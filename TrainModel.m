function [mixGaussEst] = TrainModel(data,k)

[nDim nData] = size(data);
%nDim=3 or 4 if gabor,nData=depends

%MAIN E-M ROUTINE
%there are nData data points, and there is a hidden variable associated
%with each.  If the hidden variable is 0 this indicates that the data was
%generated by the first Gaussian.  If the hidden variable is 1 then this
%indicates that the hidden variable was generated by the second Gaussian
%etc.

postHidden = zeros(k, nData);

%in the E-M algorithm, we calculate a complete posterior distribution over
%the (nData) hidden variables in the E-Step.  In the M-Step, we
%update the parameters of the Gaussians (mean, cov, w).

%we will initialize the values to random values
mixGaussEst.d = nDim;
mixGaussEst.k = k;
mixGaussEst.weight = (1/k)*ones(1,k);
mixGaussEst.mean = 2*randn(nDim,k);
for cGauss =1:k
    mixGaussEst.cov(:,:,cGauss) = (0.5+1.5*rand(1))*eye(nDim,nDim);
end;

nIter = 20;

for cIter = 1:nIter
    %Expectation step
    l=zeros(nData,mixGaussEst.k);
    for cData = 1:nData
        %Calculate posterior probability that
        %this data point came from each of the Gaussians
        thisData = data(:,cData);
        D=size(data,1);
        for k=1:mixGaussEst.k
            SigmaDet=det(mixGaussEst.cov(:,:,k));
            meanMat=mixGaussEst.mean(:,k); % 2x1
            l(cData,k)=mixGaussEst.weight(1,k)*...
                ( 1/(((2*pi)^(D/2))*(SigmaDet^(1/2))) )*(exp( -0.5*...
                (thisData-meanMat)'*(mixGaussEst.cov(:,:,k)^-1)*(thisData-meanMat) ));
        end
        postHidden(:,cData)= l(cData,:)' / sum(l(cData,:));
    end;
    
    %Maximization Step
    
    %for each constituent Gaussian
    for cGauss = 1:k
        %TO DO (h):  Update weighting parameters mixGauss.weight based on the total
        %posterior probability associated with each Gaussian. Replace this:
        %mixGaussEst.weight(cGauss) = mixGaussEst.weight(cGauss);
        mixGaussEst.weight(cGauss)= (sum(postHidden(cGauss,:)))/sum(sum(postHidden));
        %Update mean parameters mixGauss.mean by weighted average
        %where weights are given by posterior probability associated with
        %Gaussian.  
        mixGaussEst.mean(:,cGauss)=((postHidden(cGauss,:)*data')/sum(postHidden(cGauss,:)))';
        %Update covarance parameter based on weighted average of
        %square distance from update mean, where weights are given by
        %posterior probability associated with Gaussian
        mixGaussEst.cov(:,:,cGauss)=0.0;
        
        for i=1:nData
            mixGaussEst.cov(:,:,cGauss)=mixGaussEst.cov(:,:,cGauss)+...
                postHidden(cGauss,i) * ( data(:,i)-mixGaussEst.mean(:,cGauss) ) * ( data(:,i)-mixGaussEst.mean(:,cGauss) )';
        end
        mixGaussEst.cov(:,:,cGauss)= mixGaussEst.cov(:,:,cGauss)/sum(postHidden(cGauss,:));
    end;
    cIter
    %draw the new solution
    %drawEMData3d(data,mixGaussEst);
    %drawnow;
    
    %calculate the log likelihood
    %logLike = getMixGaussLogLike(data,mixGaussEst);
    %fprintf('Log Likelihood Iter %d : %4.3f\n',cIter,logLike);
    
end;

